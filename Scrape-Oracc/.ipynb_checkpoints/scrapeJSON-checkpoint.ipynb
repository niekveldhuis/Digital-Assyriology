{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ETCSRI JSON Scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "import json\n",
    "import csv\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This JSON scraper will prepare the JSON data from http://oracc.museum.upenn.edu/etcsri as training data in .csv files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get list of texts from etcsri corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "texts = dict()\n",
    "corpus_url = \"http://oracc.museum.upenn.edu/etcsri/corpus.json\"\n",
    "try:\n",
    "    corpus_data = json.loads(urlopen(corpus_url).read().decode('UTF-8'))\n",
    "    texts = corpus_data['members']\n",
    "except ValueError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "json.loads() deserializes the JSON string object, found in corpus_url, into a Python dictionary.\n",
    "We access the list of texts in the corpus_data dict by accesing the value stored at key 'members'.\n",
    "We store the corpus_data dict as a dict variable, texts, for easier access later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lines = list()\n",
    "all_lines = list()\n",
    "fragmented = True\n",
    "def recurse_keys(df, indent = '  '):\n",
    "    global fragmented\n",
    "    for key in df.keys():\n",
    "        if key == 'form':\n",
    "            if 'gw' in df.keys() and 'pos' in df.keys():\n",
    "                if df['gw'].isdigit() and 'cf' in df.keys():\n",
    "                    lines.append((df[key], df['cf'], df['pos']))\n",
    "                else:\n",
    "                    lines.append((df[key], df['gw'], df['pos']))\n",
    "                fragmented = False\n",
    "        if key == 'type':\n",
    "            if 'label' in df.keys(): \n",
    "                if df['type'] == 'line-start':\n",
    "                    if not fragmented: \n",
    "                        all_lines.append(lines[:])\n",
    "                        lines.clear()\n",
    "                    fragmented = True\n",
    "        if key == 'f':\n",
    "            recurse_keys(df[key], indent + '   ')\n",
    "        if key == 'cdl':\n",
    "            for i in range(len((df[key]))):\n",
    "                if isinstance(df[key][i], dict):\n",
    "                    recurse_keys(df[key][i], indent+'   ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "recurse_keys() is a method that will allow us to recursively traverse down the nested tree of nodes by the keys of a dict inside JSON files for individual texts. \n",
    "\n",
    "The tree three primary node types: \n",
    "1. c - a chunk of text\n",
    "2. d - a discontinuity\n",
    "3. l - a lemmatization of the text\n",
    "\n",
    "The name of the array of children of any chunk node is called \"cdl\" based on these three members. All text is stored inside the \"cdl\" elements, so we recursively call `recurse_keys` on all elements of a \"cdl\" array that are dicts to get to the bottom of the nested tree.\n",
    "\n",
    "The actual text is broken down into smaller chunks that together comprise of a bigger chunk (e.g. sentence $\\rightarrow$ discourse $\\rightarrow$ text). Each sentence may have discontinuities, and \"d\" nodes which also have \"type\" = \"line-start\" and a \"label\" element signify the beginning of a new line of text. \n",
    "\n",
    "We collect the words on one line, and when we hit a new line, we append a deep copy of the list of words in the current line, `lines`, into an `all_lines` list, and then we clear `lines` to start a new line.\n",
    "\n",
    "To collect words, we look for the key \"f\", which contains yet another dict that contains the actual word we want, as \"f\" is part of the lemmatized \"l\" node. We call `recurse_keys` on the \"f\" dict. We then look inside the \"f\" dict for the key \"form\". The value at \"form\" is a single word. We then append this word to the `lines` list that keeps track of words on the current line, along with the \"gw\" and \"pos\".\n",
    "\n",
    "The `fragmented` boolean variable is to ensure that text too fragmented for translation is not included in our training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "keys = list(texts.keys())\n",
    "vals = list(texts.values())\n",
    "for i in range(len(vals)):\n",
    "    text_url = \"http://oracc.museum.upenn.edu/etcsri/\" + vals[i]\n",
    "    try:\n",
    "        text_data = json.loads(urlopen(text_url).read().decode('UTF-8'))\n",
    "        recurse_keys(text_data)\n",
    "        all_lines.append(lines[:])\n",
    "        del all_lines[0]\n",
    "        forms = [[tup[0] for tup in line] for line in all_lines]\n",
    "        guide_words = [[tup[1] for tup in line] for line in all_lines]\n",
    "        part_of_speeches = [[tup[2] for tup in line] for line in all_lines]\n",
    "        sentences = [' '.join(line) for line in forms]\n",
    "        sentences_gw = [' '.join(line) for line in guide_words]\n",
    "        sentences_pos = [' '.join(line) for line in part_of_speeches]\n",
    "        rows = zip(sentences, sentences_gw, sentences_pos)\n",
    "        filename = 'train/etcsri/' + keys[i] + '.csv'\n",
    "        os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "        with open(filename, 'w',newline=\"\\n\", encoding=\"utf-8\") as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow(('Form', 'Gw', 'Pos'))\n",
    "            for r in rows:\n",
    "                writer.writerow(r)\n",
    "    except ValueError:\n",
    "        pass\n",
    "    lines.clear()\n",
    "    all_lines.clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We store the keys and values of the `texts` dict as separate lists `keys` and `vals`, respectively. We then iterate through the values in the dict (every text in the etcsri corpus), and deserialize the JSON string object of the individual text JSON file to a dict, `text_data`. We call `recurse_keys` on `text_data` to build up our `all_lines` list from the nested tree.\n",
    "\n",
    "We have to add the last line of text manually, since `recurse_keys` only adds a line at the next \"line-start\" node, and the last line doesn't have a next \"line-start\" node. We also have to delete the first element of `all_lines`, which is always an empty list, because `recurse_keys` adds the empty list the first time it sees a \"line-start\" node. There is no node that marks the end of the line, so I have to identify line boundaries with \"line-start\", add the missing last line, and delete the extraneous first line.\n",
    "\n",
    "We then have to flatten each `line` in `all_lines`, which is currently a list of tuples containing (form, gw, pos) in each line with the words as separate elements, into 3 separate lists for each element. This list will contain the forms/gw/pos of one sentence joined together as a string, separated by ' '. The list of these forms/gw/pos lists is stored in variables `sentences`, `sentences_gw`, and `sentences_pos`.\n",
    "\n",
    "To prepare the training data as a csv file, we write the three lists as columns of a csv file named after the current text.\n",
    "\n",
    "After we are done writing the training data of one text to the csv file of that text, we clear `lines` and `all_lines` and repeat the procedure for the rest of the texts inside the for loop. "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
